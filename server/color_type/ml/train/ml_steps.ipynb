{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model to predict a color type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "df = pd.read_csv('data/warm_cold_colors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the classes distribution skewness\n",
    "\n",
    "df[['is_warm', 'r']].groupby(['is_warm'])\\\n",
    "                    .count()\\\n",
    "                    .reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorative Data Analysis - EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes_combinations = [('r', 'g'), \n",
    "                     ('r', 'b'), \n",
    "                     ('g', 'b')]\n",
    "\n",
    "color_scale = {'Cool': 'blue', \n",
    "               'Warm':'red'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 10))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    x, y = axes_combinations[i]\n",
    "    \n",
    "    im = ax.scatter(x=df[x], \n",
    "                    y=df[y], \n",
    "                    c=df['is_warm'], \n",
    "                    cmap=matplotlib.colors.ListedColormap(color_scale.values()))\n",
    "    \n",
    "    ax.set_xlabel(x)\n",
    "    ax.set_ylabel(y)\n",
    "    \n",
    "cb = fig.colorbar(im, ax=axes.ravel().tolist(), orientation='horizontal')    \n",
    "cb.set_ticks([.25,.75])\n",
    "cb.set_ticklabels(list(color_scale.keys()))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "How close to reality a model's prediction can be; for a classificaiton problem, <a href=\"https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62\" target=\"_blank\">consufion matrix</a> is usually used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg\" alt=\"Precisionrecall.svg\" height=\"480\" width=\"264\">\n",
    "<div style=\"text-align:center\">\n",
    "By <a href=\"//commons.wikimedia.org/wiki/User:Walber\" title=\"User:Walber\">Walber</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, \n",
    "<a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=36926283\">Link</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce the flase negative and false positive rate, <em><strong>F1 score</strong></em> to be used as the model validation metric:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <font size=\"5\">\n",
    "        $F_{1} = \\frac{2}{\\frac{1}{Recall} + \\frac{1}{Precision}}$\n",
    "    </font>\n",
    "<div style=\"text-align:center\">    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <font size=\"5\">\n",
    "    $Recall = \\frac{TP}{TP+FN}$\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "describes what fraction of all true \"positive\" points did a model predict correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <font size=\"5\">\n",
    "        $Precision = \\frac{TP}{TP+FP}$\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "descibes what fraction of all predicted \"Positive\" points did a model predict correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another widely <s>used</s> abused metric, <strong><em>accuracy</em></strong>:\n",
    "<div style=\"text-align:center\">\n",
    "<font size=\"5\">\n",
    "    $Accuracy = \\frac{TP+TN}{TP+FP+TN+FN}$\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(y_true: np.array, \n",
    "                 y_pred: np.array) -> dict:\n",
    "    \"\"\"\n",
    "    Function to calculate classification metrics\n",
    "    \n",
    "        Args:\n",
    "            y_test: np.array, test subset\n",
    "            y_predicitons: np.array prediciton on x_test\n",
    "    \n",
    "        Returs:\n",
    "\n",
    "            dict with the keys:\n",
    "                confusion_matrix - confusion matrix\n",
    "                accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "                precision = TP/(TP+FP)\n",
    "                recall = TP/(TP+FN)\n",
    "                f1_score = 2/(1/recall+1/precision)    \n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(y_true) != 0, \"Empty array, please check input\"\n",
    "    \n",
    "    true_positives = np.where(y_true==1)[0]    \n",
    "    TP = sum(y_pred[true_positives]==1)\n",
    "    FN = sum(y_pred[true_positives]==0)\n",
    "    \n",
    "    true_negatives = np.where(y_true==0)[0]\n",
    "    FP = sum(y_pred[true_negatives]==1)\n",
    "    TN = sum(y_pred[true_negatives]==0)\n",
    "    \n",
    "    accuracy = (TP + TN) / len(y_true)\n",
    "    \n",
    "    recall = precision = f1_score = None\n",
    "    \n",
    "    if FN > 0 or TP > 0:\n",
    "        recall = float(TP / (TP + FN))\n",
    "        \n",
    "    if FP > 0 or TP > 0:\n",
    "        precision = float(TP / (TP + FP))\n",
    "\n",
    "    if recall and precision:\n",
    "        f1_score = 2/(1/recall+1/precision)    \n",
    "        \n",
    "    return {\"confusion_matrix\": {\n",
    "                \"TP\": TP, \"FP\": FP, \n",
    "                \"FN\": FN, \"TN\": TN\n",
    "            },\n",
    "            \"accuracy\": accuracy,\n",
    "            \"recall\": recall,\n",
    "            \"precision\": precision,\n",
    "            \"f1_score\": f1_score\n",
    "           } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As a baseline, we can set the condition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```javascript\n",
    "if r > g > b:\n",
    "    color_type = 'Warm'\n",
    "else:\n",
    "    color_type = 'Cool'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_baseline:\n",
    "    \"\"\"\n",
    "    Baseline model\n",
    "    \"\"\"\n",
    "\n",
    "    def predict(X: pd.DataFrame) -> np.array:\n",
    "        \"\"\"\n",
    "        Function to run a base line prediction\n",
    "\n",
    "            Args:\n",
    "                X: input data\n",
    "\n",
    "            Returns:\n",
    "                array\n",
    "        \"\"\"\n",
    "        def _rule(row: pd.DataFrame) -> int:\n",
    "            \"\"\"\n",
    "            Model rule\n",
    "                Args:\n",
    "                    Row: pd.DataFrame row\n",
    "\n",
    "                Return:\n",
    "                    int\n",
    "            \"\"\"\n",
    "\n",
    "            if row['r'] > row['g'] > row['b']:\n",
    "                return 1\n",
    "\n",
    "            return 0\n",
    "\n",
    "        return X.apply(lambda row: _rule(row), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model\n",
    "model_v1 = model_baseline()\n",
    "\n",
    "y_predict_baseline = model_v1.predict(df[['r', 'g', 'b']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model evaluation\n",
    "\n",
    "eval_metrics(df['is_warm'], y_predict_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So without any machine learning, we were managed to get the F1 score of <strong>0.74</strong> :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assess out \"model\" using a random data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test point -> a color of the \"Cool\" type/class 0\n",
    "\n",
    "test_point = pd.DataFrame({'r': [8], 'g': [103], 'b': [203]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1.predict(test_point).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deploy the baseline model as a POC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can improve our ML service by improving our model's accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=seed)\\\n",
    "       .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('is_warm', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scale.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_scaled, df['is_warm'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier as xgb_class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.drop('is_warm', axis=1), df['is_warm'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\": 'binary:logistic',\n",
    "    \"learning_rate\": 0.5, \n",
    "    \"n_estimators\": 100, \n",
    "    \"max_depth\": 3,\n",
    "    \"n_jobs\": 4,\n",
    "    \"silent\": False, \n",
    "    \"subsample\": 0.8,\n",
    "    \"random_state\": seed\n",
    "}\n",
    "\n",
    "model_v2 = xgb_class(**params)\n",
    "\n",
    "model_v2.fit(x_train, y_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_xgb = model_v2.predict(df.drop('is_warm', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics(df['is_warm'], y_predict_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point test\n",
    "\n",
    "model_v2.predict(test_point).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>0.97</strong> is quite an improvement, so let's dump the model and re-deploy the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    \"\"\"\n",
    "    Function to save/pickle python object\n",
    "\n",
    "        Args:\n",
    "            filename: str path to pickle file\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(model_v2, '../model/v2/model_v2.sav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
