{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, GridSearchCV\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(y_true: list, y_pred: list) -> dict:\n",
    "    \"\"\"\n",
    "    Function to calculate the classification metrics\n",
    "    \n",
    "        Args:\n",
    "            y_test: list, test subset\n",
    "            y_predicitons: list prediciton on x_test\n",
    "    \n",
    "        Returs:\n",
    "\n",
    "            dict with the keys:\n",
    "                confusion_matrix - confusion matrix\n",
    "                accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "                precision = TP/(TP+FP)\n",
    "                recall = TP/(TP+FN)\n",
    "                f1_score = 2/(1/recall+1/precision)\n",
    "                specifisity = TN/(TN+FP)\n",
    "                npv = TN/(TN+FN)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # total numb of true and false labels\n",
    "    labels_real = cm.sum(axis = 1)\n",
    "    labels_pred = cm.sum(axis = 0)\n",
    "    n_correct_labels = cm.diagonal().sum()\n",
    "    accuracy = n_correct_labels/len(y_true)\n",
    "    recall = cm[1,1]/labels_real[1]\n",
    "    precision = cm[1,1]/labels_pred[1]    \n",
    "    nvp = cm[0,0]/labels_pred[0]\n",
    "    specificity = cm[0,0]/labels_real[0]\n",
    "        \n",
    "    return {\"confusion_matrix\": cm,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"recall\": recall,\n",
    "            \"precision\": precision,\n",
    "            \"f1_score\": 2/(1/recall+1/precision),\n",
    "            \"specificity\": specificity,\n",
    "            \"npv\":nvp\n",
    "           } \n",
    "\n",
    "def model_eval(x_train, y_train, \n",
    "              x_test, y_test, \n",
    "              model):\n",
    "    \"\"\"\n",
    "    Function to evaluate model\n",
    "    \"\"\"\n",
    "    y_train_predict = model.predict(x_train)\n",
    "    y_test_predict = model.predict(x_test)\n",
    "    \n",
    "    # metrics\n",
    "    metrics_train = eval_metrics(y_train, y_train_predict)\n",
    "    metrics_test = eval_metrics(y_test, y_test_predict)\n",
    "    \n",
    "    return metrics_train, metrics_test\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    \"\"\"\n",
    "    Function to save/pickle python object\n",
    "\n",
    "        Args:\n",
    "            filename: str path to pickle file\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def read_object(filename: str):\n",
    "    \"\"\"\n",
    "    Function to read/un-pickle python object\n",
    "\n",
    "        Args:\n",
    "            filename: str path to pickle file\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, 'rb') as input_stream:\n",
    "        obj = pickle.load(input_stream)\n",
    "    return obj\n",
    "\n",
    "def point_test(test_point: dict, model) -> bool:\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to run a prediciton on a data point\n",
    "    \"\"\"\n",
    "    try:\n",
    "        is_warm = model.predict(pd.DataFrame(test_point)).squeeze()\n",
    "        return bool(is_warm)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 136 entries, 0 to 135\n",
      "Data columns (total 4 columns):\n",
      "r          136 non-null int64\n",
      "g          136 non-null int64\n",
      "b          136 non-null int64\n",
      "is_warm    136 non-null int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 4.3 KB\n"
     ]
    }
   ],
   "source": [
    "# read the data to train the model\n",
    "dat = pd.read_csv('data/warm_cold_colors.csv')\n",
    "\n",
    "dat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r</th>\n",
       "      <th>g</th>\n",
       "      <th>b</th>\n",
       "      <th>is_warm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>254</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221</td>\n",
       "      <td>39</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>254</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>254</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>255</td>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     r    g    b  is_warm\n",
       "0  254   37    0        1\n",
       "1  221   39  123        1\n",
       "2  254   89    0        1\n",
       "3  254  153    0        1\n",
       "4  255  221    0        1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshuffle data\n",
    "seed = 2019\n",
    "\n",
    "dat = dat.sample(len(dat), random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(dat.drop(columns='is_warm'), dat['is_warm'], \n",
    "                                                    test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test point -> should be cool, 0 class\n",
    "\n",
    "test_point = {'r': [8], 'g': [103], 'b': [203]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {    \n",
    "    \"weights\": [\"distance\", \"uniform\"], \n",
    "    \"algorithm\" : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    \"n_neighbors\": [i for i in range(3,11,1)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(KNeighborsClassifier(), params, scoring='f1', iid=False, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.15 s, sys: 42 ms, total: 4.19 s\n",
      "Wall time: 4.56 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=False, n_jobs=None,\n",
       "       param_grid={'weights': ['distance', 'uniform'], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], 'n_neighbors': [3, 4, 5, 6, 7, 8, 9, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.99153367, 0.90600484, 0.99153367, 0.88277085, 0.99153367,\n",
       "       0.87868535, 0.99153367, 0.86454471, 0.99153367, 0.87905628,\n",
       "       0.99153367, 0.85599998, 0.99153367, 0.85502744, 0.99153367,\n",
       "       0.85426755, 0.99153367, 0.90600484, 0.99153367, 0.88277085,\n",
       "       0.99153367, 0.87868535, 0.99153367, 0.86454471, 0.99153367,\n",
       "       0.87905628, 0.99153367, 0.85599998, 0.99153367, 0.85502744,\n",
       "       0.99153367, 0.85426755, 0.99153367, 0.90600484, 0.99153367,\n",
       "       0.88277085, 0.99153367, 0.87868535, 0.99153367, 0.86454471,\n",
       "       0.99153367, 0.87905628, 0.99153367, 0.85599998, 0.99153367,\n",
       "       0.85502744, 0.99153367, 0.85426755, 0.99153367, 0.90640486,\n",
       "       0.99153367, 0.88277085, 0.99153367, 0.87868535, 0.99153367,\n",
       "       0.86454471, 0.99153367, 0.87905628, 0.99153367, 0.85599998,\n",
       "       0.99153367, 0.85502744, 0.99153367, 0.85426755])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_score['mean_train_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_train, metrics_test = model_eval(x_train, y_train, x_test, y_test, knn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8823529411764706"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_train['f1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7692307692307692"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_test['f1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(knn_model, 'model/model_v0.knn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_test(test_point, knn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier as xgb_class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.5, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=4, nthread=None, objective='binary:logistic',\n",
       "       random_state=2019, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=False, subsample=0.8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init the model and train it\n",
    "\n",
    "params = {\n",
    "    \"objective\": 'binary:logistic',\n",
    "    \"learning_rate\": 0.5, \n",
    "    \"n_estimators\": 100, \n",
    "    \"max_depth\": 3,\n",
    "    \"n_jobs\": 4,\n",
    "    \"silent\": False, \n",
    "    \"subsample\": 0.8,\n",
    "    \"random_state\": seed\n",
    "}\n",
    "\n",
    "model_xgb = xgb_class(**params)\n",
    "\n",
    "model_xgb.fit(x_train, y_train, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_train, metrics_test = model_eval(x_train, y_train, x_test, y_test, model_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9904761904761905"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_train['f1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_test['f1_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_test(test_point, model_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(model_xgb, 'model/model_v1.xgb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
